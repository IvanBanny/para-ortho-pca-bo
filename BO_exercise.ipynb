{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Sheet 4\n",
    "\n",
    "## Part 1: Gaussian Process Regression\n",
    "\n",
    "### Library Loading Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T09:45:53.131414Z",
     "start_time": "2025-04-18T09:45:51.760678900Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyDOE2'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 20\u001B[39m\n\u001B[32m     18\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msklearn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mgaussian_process\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mkernels\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Matern,RBF, ConstantKernel \u001B[38;5;28;01mas\u001B[39;00m C\n\u001B[32m     19\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtyping\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Callable\n\u001B[32m---> \u001B[39m\u001B[32m20\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpyDOE2\u001B[39;00m\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'pyDOE2'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thursday October 3, 2024\n",
    "\n",
    "@authors: Elena Raponi, Kirill Antonov, Ivan Olarte-Rodriguez\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Import the Libraries\n",
    "\n",
    "# Typical Libraries for Numerical Processing\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Scikit Learn Libraries (For handling basic Regression Processes)\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern,RBF, ConstantKernel as C\n",
    "from typing import Callable\n",
    "import pyDOE2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following block there are listed four different functions. Run this block in order to load the functions into the main module. These functions are then going to be switched to perform different approximations with the regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T09:45:53.147933100Z",
     "start_time": "2025-04-18T09:45:53.137444200Z"
    }
   },
   "outputs": [],
   "source": [
    "def f(X:np.ndarray):\n",
    "    # 1D example function to illustrate Gaussian process\n",
    "    x = X.copy()\n",
    "    return x * np.cos(x) + 10\n",
    "\n",
    "def Ackley(x1:np.ndarray, x2:np.ndarray):\n",
    "    # Usually evaluated on [-5,5]^2\n",
    "    # Global minimum at (0, 0) where f(x1, x2) = 0\n",
    "    return -20*np.exp(-0.2*np.sqrt(0.5*(x1**2+x2**2))) - np.exp(0.5*(np.cos(2*np.pi*x1)+np.cos(2*np.pi*x2))) + 20 + np.e\n",
    "\n",
    "def Rosenbrock(x1:np.ndarray, x2:np.ndarray): \n",
    "    # Usually evaluated on [-2,2]^2\n",
    "    # Global minimum at (a,a^2) where f(x1,x2)=0\n",
    "    a = 1\n",
    "    b = 100     \n",
    "    return (a-x1)**2 + b*(x2 - x1**2)**2\n",
    "\n",
    "def Quadratic(x1:np.ndarray, x2:np.ndarray):\n",
    "    # Can be evaluated on any search domain\n",
    "    # Global minimum at (0,0) where f(x1,x2)=0\n",
    "    return x1**2 + x2**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following block to plot the implementation of the interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T09:45:53.642680900Z",
     "start_time": "2025-04-18T09:45:53.147933100Z"
    }
   },
   "outputs": [],
   "source": [
    "# define our known points and their measurements\n",
    "# We start with a 1D simple example\n",
    "# Sampling locations\n",
    "xi = np.atleast_2d([-3., -1, 1.5, 2.5, 3]).T\n",
    "\n",
    "# Sample observations\n",
    "fi = f(xi).ravel()\n",
    "\n",
    "# Some length scale\n",
    "L= 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block is a helper block to generate plotting functions. These are meant to plot the function, the prediction and the 95% confidence interval based on the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-18T09:45:53.148937300Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_results_2d(X_obs, y_obs, x, y_pred, sigma):\n",
    "    plt.figure()\n",
    "    plt.plot(x, f(x), 'r:', label='$f(x) = x\\ cos(x) + 10$')\n",
    "    plt.plot(X_obs, y_obs, 'r.', markersize=10, label='Observations')\n",
    "    plt.plot(x, y_pred, 'k-', label='Prediction')\n",
    "    plt.fill(np.concatenate([x, x[::-1]]),\n",
    "             np.concatenate([y_pred - 1.9600 * sigma,\n",
    "                            (y_pred + 1.9600 * sigma)[::-1]]),\n",
    "             alpha=.3, fc='k', label='95% confidence interval')\n",
    "    plt.xlabel('$x$')\n",
    "    plt.ylabel('$f(x)$')\n",
    "    plt.ylim(0, 25)\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.5, 1))\n",
    "    plt.show()\n",
    "    \n",
    "def plot_doe(x, lb = 0.0, ub = 1.0):\n",
    "    num_samp = np.size(x,0)\n",
    "    num_sampf = float(num_samp)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca()\n",
    "    ax.scatter(x[:,0],x[:,1])\n",
    "    # --- boundaries of the design space\n",
    "    ax.plot([0,0,1,1,0],[0,1,1,0,0], color=[0.5,0.5,0.5], linestyle='--', linewidth = 0.5)\n",
    "    # --- grid lines  \n",
    "    for i in range(num_samp):\n",
    "        tmp = lb + (ub-lb)*i/num_sampf\n",
    "        ax.plot([lb,ub],[tmp,tmp],color=[0.5,0.5,0.5], linestyle=':')\n",
    "        ax.plot([tmp,tmp],[lb,ub],color=[0.5,0.5,0.5], linestyle=':')\n",
    "        \n",
    "    ax.set_xlim(lb,ub)\n",
    "    ax.set_ylim(lb,ub)\n",
    "    # -- labels\n",
    "    ax.set_xlabel('x1', labelpad=8)\n",
    "    ax.set_ylabel('x2', labelpad=8)\n",
    "    plt.title('Optimal LH Samples', fontsize = 12)\n",
    "    plt.axis('square')\n",
    "    \n",
    "def plot_results_3d(x1, x2, y_pred, x_samp=None, y_samp=None, lb=0.0, ub=1.0, title = '3D surface plot'):\n",
    "    \n",
    "    plt.figure(figsize=(15,10))\n",
    "\n",
    "    fig,ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "    ax.plot_surface(x1, x2, y_pred, cmap='coolwarm', alpha = 0.6)\n",
    "    if x_samp is not None:\n",
    "        ax.scatter(x_samp[:,0], x_samp[:,1], y_samp, c='r', marker='x')\n",
    "    ax.set_xlim(lb,ub)\n",
    "    ax.set_ylim(lb,ub)\n",
    "    ax.set_xlabel('x1')\n",
    "    ax.set_ylabel('x2')\n",
    "    ax.set_zlabel('Obj. Function')\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Gaussian Process Regression\n",
    "Fit the gaussian process regression (GPR) model using Scikit Learn's class `GaussianProcessRegressor`. Sample points and kernel definition are given. Define all the input parameters required by `plot_results_2d`.\n",
    "\n",
    "In the following setting, the final model is obtained by using numerical optimization techniques to maximize the likelihood of the observed data, given our GPR model. Set the number of restarts for the optimizer to 10. Furthermore, in the following block you may find the definition of two kernels from `scikit-learn`, namely a composite kernel comprising a RBF and a constant kernel and a Mat√©rn kernel. In addition to this, try changing the length scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-18T09:45:53.149934500Z"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    # We start with a 1D simple example\n",
    "    # Sampling locations\n",
    "    X:np.ndarray = xi.copy()\n",
    "\n",
    "    # Sample observations\n",
    "    y = fi.copy()\n",
    "\n",
    "    # Mesh the input space for evaluations of the real function, the prediction and confidence interval\n",
    "    x = np.atleast_2d(np.linspace(-5, 5, 1000)).T\n",
    "\n",
    "    # Initiate a Gaussian Process model\n",
    "    # RBF stands for Radial-basis function kernel (i.e. squared-exponential kernel\n",
    "    # Other available kernels include: C (for constant), WhiteKernel (for white noise), \n",
    "    #                                  Matern, RationalQuadratic, ExpSineSquared (for a periodic kernel)\n",
    "    # See https://scikit-learn.org/stable/modules/gaussian_process.html#kernels-for-gaussian-processes\n",
    "    # for detailed documentation of all available kernels\n",
    "    \n",
    "    ##### TODO: In this part switch between the following defined kernels. Evaluate the results by switching each of them\n",
    "    #kernel = C(1.0, (1e-3, 1e3)) * RBF(length_scale=L)\n",
    "    kernel = Matern(length_scale=L/1/3,nu=2.5)\n",
    "\n",
    "    #####--------------- END OF SWITCH -----------------------\n",
    "    \n",
    "    ##### TODO: Enter your code here:\n",
    "    \n",
    "    gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)\n",
    "\n",
    "    # Fit GPR model to observations (--> Maximum Likelihood Estimation)\n",
    "    gp.fit(X, y)\n",
    "\n",
    "    # Make the prediction on the meshed x-axis (ask for standard deviation as well)\n",
    "    y_pred, sigma = gp.predict(x, return_std=True)\n",
    "    \n",
    "    ##### --------------- END OF CODE INPUT -----------------------\n",
    "\n",
    "    \n",
    "    plot_results_2d(X,y,x,y_pred,sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #2: Two-dimensional Gaussian Process Regression\n",
    "Bring all your knowledge together and implement the whole process, from DoE to GPR regression, in this 2D example, i.e. we work with 2 variables and fit a response surface in a 3-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-18T09:45:53.151497500Z"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    # Set parameters\n",
    "    g = Rosenbrock # options are Quadratic, Ackley, Rosenbrock\n",
    "    num_dv = 2\n",
    "    num_samp = 200  # Usually 10*[number of design variables] is a good rule of thumb\n",
    "    lb = -2\n",
    "    ub = 2\n",
    "    \n",
    "    ##### TODO: Enter your code here:\n",
    "    \n",
    "    # Create samples using Optimal Latin Hypercube approach\n",
    "    x_samp = pyDOE2.lhs(num_dv, num_samp, criterion = 'centermaximin', iterations = 50000)\n",
    "    x_samp = lb + (ub-lb)*x_samp  # We have to map the generated samples back to our design space\n",
    "    plot_doe(x_samp, lb, ub)\n",
    "    \n",
    "    # Calculate objective function values\n",
    "    y_samp = g(x_samp[:,0], x_samp[:,1])\n",
    "\n",
    "    #Initiate Gaussian Process model\n",
    "    kernel = C(1.0, (1e-3, 1e3)) * RBF(length_scale=1, length_scale_bounds=(1e-2, 1e2))\n",
    "    gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)\n",
    "    \n",
    "    # Fit GPR model to observations (--> Maximum Likelihood Estimation)\n",
    "    gp.fit(x_samp, y_samp)\n",
    "    \n",
    "    #Predict values on a meshed input space\n",
    "    x1_obs, x2_obs = np.meshgrid(np.linspace(lb,ub,100), np.linspace(lb,ub,100))\n",
    "    x_obs = np.array([np.reshape(x1_obs, (1,np.prod(x1_obs.shape)))[0], \\\n",
    "                      np.reshape(x2_obs, (1,np.prod(x2_obs.shape)))[0]]).T\n",
    "    \n",
    "    y_pred = gp.predict(x_obs)\n",
    "    y_pred = np.reshape(y_pred, (100,100))\n",
    "    \n",
    "    ##### --------------- END OF CODE INPUT -----------------------\n",
    "    \n",
    "    #Visualize results\n",
    "    plot_results_3d(x1_obs, x2_obs, y_pred, x_samp, y_samp, lb=lb, ub=ub,\\\n",
    "                   title='Gaussian Process Regressor model of objective function')\n",
    "    \n",
    "    #Also plot original function for comparison\n",
    "    plot_results_3d(x1_obs, x2_obs, g(x1_obs,x2_obs), lb=lb, ub=ub,\\\n",
    "                   title='True objective function')\n",
    "    \n",
    "    #Finally get the difference between original and fitted surface\n",
    "    plot_results_3d(x1_obs, x2_obs, y_pred - g(x1_obs,x2_obs), lb=lb, ub=ub,\\\n",
    "                   title='Difference between true and fitted surface')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #3: Implement a Kernel Extension and predict functions\n",
    "In the following blocks you're encouraged to generate your own tailormade Kernel and Predict functions. In the next block, you're encouraged to write a Matern Kernel yourself. For this, we will extend the abstract kernel definition found in `scikit-learn` (https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.Kernel.html#sklearn.gaussian_process.kernels.Kernel). In this setting a fixed member variable is needed, namely a `length scale` definition. On the other hand, the Matern Kernel requires parameters `nu` and `alpha_0`.\n",
    "\n",
    "In the following block implement the `__call__` function. The steps are commented as a hint.\n",
    "\n",
    "Specifically, the Matern Kernel is defined as:\n",
    "\n",
    "$$K(x,x^{\\prime}|\\alpha_{0},\\nu) = \\alpha_{0} \\frac{2^{1-\\nu}}{\\Gamma(\\nu)} \\left(  \\sqrt{2 \\nu} \\ d(x,x^{\\prime})   \\right)^{\\nu} J_{(\\nu)} \\left(  \\sqrt{2 \\nu} \\ d(x,x^{\\prime})    \\right)$$ \n",
    "\n",
    "Where $d(x,x^{\\prime}) $ is a distance metric between two points in the parameter space. Given that the space is finite dimensional, then this distance is normally the Euclidean distance (or __2-norm__)factored by a length scale $L$ as:\n",
    "\n",
    "$$ d(x,x^{\\prime})  = \\frac{||x - x^{\\prime} ||_{2}}{L}$$\n",
    "\n",
    "On the other hand $\\Gamma(\\nu)$ represents the Gamma function evaluated at $\\nu$. This function is evaluated via `scipy.special.gamma`. $J_{(\\nu)}$ is the modified Bessel function of the second kind parameterized by $\\nu$. This is evaluated via the function `scipy.special.kv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-18T09:45:53.155452500Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.gaussian_process.kernels import Kernel\n",
    "from scipy.special import kv, gamma\n",
    "\n",
    "class CustomMaternKernel(Kernel):\n",
    "    def __init__(self, length_scale:float, nu:float=1.5, alpha_0:float = 1.0):\n",
    "        ''' \n",
    "        This initialization requires the definition of and length scale (as default from Kernel framework)\n",
    "        as well as the parameters of the Matern kernel (nu and alpha_0). In this case (to simplify the implementation)\n",
    "        this kernel should be isotropic, meaning that the length scale is a scalar value \n",
    "        '''\n",
    "        self.__length_scale:float = length_scale\n",
    "        self.__nu:float = nu\n",
    "        self.__alpha_0:float = alpha_0\n",
    "\n",
    "        \n",
    "    def __call__(self, x1:np.ndarray, x2:np.ndarray=None)->np.ndarray:\n",
    "        ''' \n",
    "        The call function Requires x1 and x2 to each represent points in the parametric space. \n",
    "        The size of x1 should be (n1 x d) and x2 should be (n2 x d), where d stands for the dimensionality of the problem. \n",
    "\n",
    "        The output of this call function must be a n1 x n2 matrix. The matrix has the computation of kernel(x1[ii,:],x2[jj,:]), where ii in range(x1.shape[0]) \n",
    "        and jj in range(x2.shape[0])\n",
    "        '''\n",
    "        if x2 is None:\n",
    "            x2 = x1.copy()\n",
    "\n",
    "    \n",
    "        matrix = np.zeros((x1.shape[0],x2.shape[0]))\n",
    "\n",
    "        ##### TODO: Enter your code here:\n",
    "        for ii in range(matrix.shape[0]):\n",
    "            for jj in range(matrix.shape[1]):\n",
    "\n",
    "                # =========Compute the variable r: ==========================\n",
    "                # r is the Euclidean distance between x1 and x2,\n",
    "                # after we normalize each dimension by the length scale\n",
    "                r = np.linalg.norm(np.divide(x1[ii,:]-x2[jj,:],self.__length_scale),ord=2)\n",
    "\n",
    "\n",
    "                # ========= Adjust the case when r=0: ==========================\n",
    "                # When r is 0 (which causes tmp to be 0),\n",
    "                # the code below returns nan because tmp**nu is 0 but kv(nu,tmp) is infinite.\n",
    "                # We fix this by adding a small amount to r in this situation.\n",
    "                # We could have also fixed this by simply returning alpha0,\n",
    "                # which is the correct covariance when r/0.\n",
    "                if r==0.:\n",
    "                    r += np.finfo(float).eps\n",
    "\n",
    "                # ========= Compute the formula for the Matern Kernel: ==========================\n",
    "                tmp =   np.sqrt(2*self.nu)*r\n",
    "                matrix[ii,jj] = self.__alpha_0 * self.c * tmp**self.__nu *(self.__nu, tmp)\n",
    "\n",
    "        ##### --------------- END OF CODE INPUT -----------------------\n",
    "\n",
    "        return matrix \n",
    "\n",
    "    \n",
    "        \n",
    "    ### These are methods required by the implementation\n",
    "    def diag(self, X:np.ndarray):\n",
    "        if len(X.shape)==1:\n",
    "            np.expand_dims(X,axis=1)\n",
    "            \n",
    "        r = np.zeros(X.shape[0]) + np.finfo(float).eps\n",
    "        \n",
    "        tmp =   np.sqrt(2*self.__nu)*r\n",
    "        return self.__alpha_0 * self.c * tmp**self.__nu * kv(self.__nu, tmp)\n",
    "\n",
    "    def is_stationary(self):\n",
    "        return True\n",
    "    \n",
    "    @property\n",
    "    def alpha_0(self)->float:\n",
    "        return self.__alpha_0\n",
    "    \n",
    "    @property\n",
    "    def nu(self)->float:\n",
    "        return self.__nu\n",
    "    \n",
    "    @property\n",
    "    def length_scale(self)->float:\n",
    "        return self.__length_scale\n",
    "    \n",
    "    @alpha_0.setter\n",
    "    def alpha_0(self,new_alpha_0:float)->None:\n",
    "        self.__alpha_0 = new_alpha_0\n",
    "\n",
    "    @nu.setter\n",
    "    def nu(self,new_nu:float)->None:\n",
    "        self.__nu = new_nu\n",
    "\n",
    "    @length_scale.setter\n",
    "    def length_scale(self,new_length_scale:float)->None:\n",
    "        self.__length_scale = new_length_scale\n",
    "    \n",
    "    @property\n",
    "    def c(self)->float:\n",
    "        return   2**(1-self.__nu) / gamma(self.__nu)\n",
    "    \n",
    "\n",
    "# Call an instance of your customized kernel\n",
    "kernel = CustomMaternKernel(length_scale=1/L,alpha_0=0.5,nu=1.5)\n",
    "\n",
    "# Print a computation\n",
    "X = np.array([[1, 2], [3, 4]])\n",
    "X2 = np.array([[2, 3], [4, 5]]) kv\n",
    "print(kernel(X,X2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the following block to test your customized Kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-18T09:45:53.158455200Z"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    # We start with a 1D simple example\n",
    "    # Sampling locations\n",
    "    X:np.ndarray = xi.copy().reshape((-1, 1))\n",
    "\n",
    "\n",
    "    # Sample observations\n",
    "    y = fi.copy().reshape((-1, 1))\n",
    "    \n",
    "\n",
    "    # Mesh the input space for evaluations of the real function, the prediction and confidence interval\n",
    "    x = np.atleast_2d(np.linspace(-5, 5, 1000)).reshape((-1, 1))\n",
    "\n",
    "    # Initiate a Gaussian Process model\n",
    "    kernel = CustomMaternKernel(length_scale=1/L,nu=1.5)\n",
    "    gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)\n",
    "\n",
    "    # Fit GPR model to observations (--> Maximum Likelihood Estimation)\n",
    "    gp.fit(X, y)\n",
    "\n",
    "    # Make the prediction on the meshed x-axis (ask for standard deviation as well)\n",
    "    y_pred, sigma = gp.predict(x, return_std=True)\n",
    "    \n",
    "    \n",
    "    plot_results_2d(X,y,x.reshape((-1,1)),y_pred.reshape((-1,1)),sigma.reshape((-1,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's turn to call a customized `fit_predict` function. Write a function that calculates the mean and variance of the \n",
    "posterior distribution at a point test_x. Your function should \n",
    "return the result as a list, where the first entry  is the posterior mean\n",
    "and the second is the posterior variance.  You may find that because of \n",
    "numerical imprecision, the posterior variance can be slightly negative.\n",
    "\n",
    "The mean and variance of the posterior distribution are defined as:\n",
    "\\begin{align}\n",
    "f(x) &\\mid f(x_{1:n}) \\sim \\mathrm{Normal}(\\mu_n(x),\\sigma^2_n(x))\\\\\n",
    "\\mu_n(x) &= \\Sigma_0(x,x_{1:n}) \\Sigma_0(x_{1:n},x_{1:n})^{-1} (f(x_{1:n})-\\mu_0(x_{1:n})) + \\mu_0(x) \\\\\n",
    "\\sigma^2_n(x) &= \\Sigma_0(x,x) - \\Sigma_0(x,x_{1:n}) \\Sigma_0(x_{1:n},x_{1:n})^{-1} \\Sigma_0(x_{1:n},x)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "For the solution we suggest two linear algebra functions from numpy:\n",
    "- numpy.linalg.inv: https://numpy.org/doc/stable/reference/generated/numpy.linalg.inv.html\n",
    "- numpy.matmul: https://numpy.org/doc/stable/reference/generated/numpy.matmul.html\n",
    "\n",
    "*Hint 1*: The formula in the BayesOpt tutorial has the expression,\n",
    "$\\Sigma_0(x_{1:n},x)$\n",
    "which is a short way of writing the column vector,\n",
    "$[\\Sigma_0(x_1,x), \\ldots, \\Sigma_0(x_n,x)]^T$.\n",
    "\n",
    "Suppose we have python code where:\n",
    "* $\\Sigma_0$ is computed by the function `kernel`\n",
    "* $x_{1:n}$ is stored in the list `train_x`\n",
    "* We have another point `x`\n",
    "\n",
    "So then the row vector $\\Sigma_0(x_{1:n},x)$ we want to construct is:\n",
    "\n",
    "    [kernel(train_x[0],x),..., kernel(train_x[n-1],x))]\n",
    "\n",
    "A quick way to construct this row vector as a list is \n",
    "\n",
    "    [kernel(y,x) for y in train_x]\n",
    "\n",
    "*Hint 2:* You can convert a list of numbers to a numpy array. This allows for matrix multiplication, taking the transpose, and adding a subtracting scalars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-18T09:45:53.161974800Z"
    }
   },
   "outputs": [],
   "source": [
    "# This defines a mean function that evaluates to 0 for all inputs \n",
    "mean0 = lambda x : 0.0\n",
    "kern = CustomMaternKernel(length_scale=1/L,alpha_0=0.5,nu=1.5)\n",
    "\n",
    "\n",
    "def fit_predict(test_x, train_x, train_y, mean = mean0, kernel=kern):\n",
    "    # Inputs:\n",
    "    # test_x is a point in the x space\n",
    "    # train_x is an array of points\n",
    "    # train_y is an array equal in length to train_x, giving the value of our observation at each point\n",
    "    # mean and kernel are two functions\n",
    "    # mean should take one point as input, mean(x)\n",
    "    # kernel should take two points as input, kernel(x,xprime)\n",
    "    \n",
    "    ##### TODO:  Enter your code here:\n",
    "    \n",
    "    # An array with the mean\n",
    "    mu_v = np.vectorize(mean) \n",
    "    mu0 = mu_v(train_x)\n",
    "\n",
    "    if not isinstance(test_x,np.ndarray):\n",
    "        test_x = np.array([test_x]).reshape((-1,1))\n",
    "\n",
    "    # K is the matrix kernel(train_x,train_x)\n",
    "    K = kernel(train_x)\n",
    "    Kinv = np.linalg.inv(K)\n",
    "\n",
    "    # A is the vector kernel(test_x,train_x), which is 1 by n\n",
    "    # B is its product with K^-1\n",
    "    \n",
    "    A = kernel(test_x,train_x)\n",
    "    B = np.matmul(A,Kinv)\n",
    "\n",
    "    pred_mean = np.matmul(B,train_y-mu0) + mu_v(test_x).reshape((-1,1))\n",
    "    pred_var = kernel(test_x,test_x) - np.matmul(B,np.transpose(A))\n",
    "    \n",
    "    ##### --------------- END OF CODE INPUT -----------------------\n",
    "    \n",
    "    return (pred_mean, pred_var)\n",
    "    \n",
    "    \n",
    "# Here are some examples of what your function should return.\n",
    "# The posterior variance is very low for the first example because we are asking\n",
    "# for a prediction for a point that is in the training data. \n",
    "print(fit_predict(0.9, X.reshape((-1, 1)), y.reshape((-1, 1))))\n",
    "print(fit_predict(2, X.reshape((-1, 1)), y.reshape((-1, 1))))\n",
    "print(fit_predict(-2, X.reshape((-1, 1)), y.reshape((-1, 1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Acquisition Functions\n",
    "In this section of the exercise you will now work with the acquisition functions. In the framework of Bayesian Optimization, you first need to build a surrogate model through a Gaussian Process Regression. With the information of the regressor, then you need to decide the next point of the decision space to pick. This decision process is evaluated through an acquisition function which points the \"next most interesting point\" according to some metric embedded into the acquistion function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-18T09:45:53.164300100Z"
    }
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import math\n",
    "# Read in data from a file.  Download this from the BrightSpace  before you start.  \n",
    "# If you forget to do this, the cell will generate the data for you, but small\n",
    "# differences in random number generators might lead to different training data,\n",
    "# which may make the lab harder to do.\n",
    "filename = 'lab1_data.csv'\n",
    "\n",
    "# If data doesn't exist, generate it\n",
    "if not os.path.exists(filename):\n",
    "    np.random.seed(1)\n",
    "    train_x = np.sort(np.random.rand(10)) # 10 points, uniformly distributed between 0 and 1\n",
    "    train_y = [math.sin(x * (2 * math.pi)) + 0.0 * np.random.randn() for x in train_x]\n",
    "    data = np.transpose([train_x,train_y])\n",
    "    np.savetxt(filename,data)\n",
    "\n",
    "# Read in data from a file.  \n",
    "data = np.loadtxt(filename)\n",
    "train_x = data[:,0] # First column of the data\n",
    "train_y = data[:,1] # Second column of the data\n",
    "plt.xlim(0,1)\n",
    "plt.plot(train_x, train_y,'o-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-18T09:45:53.165300600Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_prediction(train_x, train_y, gpr:GaussianProcessRegressor):\n",
    "    m = []\n",
    "    lb = []\n",
    "    ub = []\n",
    "\n",
    "    train_x = train_x.reshape((-1,1))\n",
    "    train_y = train_y.reshape((-1,1))\n",
    "\n",
    "    # Fit the GPR\n",
    "    gpr.fit(train_x,train_y)\n",
    "\n",
    "    x = np.linspace(0,1,100)\n",
    "    for test_x in x:\n",
    "        pred_mean, pred_std = gpr.predict(np.array([test_x]).reshape(-1,1),return_std=True)\n",
    "        m.append(pred_mean)\n",
    "    \n",
    "        # Calculate the posterior standard deviation\n",
    "        if pred_std<0:\n",
    "            print('Posterior standard deviation is negative ({:e}) at x={}, rounding up to 0'.format(pred_std, test_x))\n",
    "            pred_std = 0\n",
    "    \n",
    "        s = pred_std.copy() \n",
    "        \n",
    "        lb.append(pred_mean-1.96*s)\n",
    "        ub.append(pred_mean+1.96*s)\n",
    "    \n",
    "    m = np.array(m).ravel()\n",
    "    lb = np.array(lb).ravel()\n",
    "    ub = np.array(ub).ravel()\n",
    "\n",
    "    _, ax = plt.subplots(figsize=(6.5, 4))\n",
    "    ax.plot(train_x,train_y,'ko')\n",
    "    ax.plot(x,m.ravel(),'k',linewidth=0.5)\n",
    "    ax.fill_between(x,lb,ub)\n",
    "    ax.legend(['Observed Data', 'Mean', 'Confidence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: The Expected Improvement Function\n",
    "The Expected Improvement is the most used acquisition function in most Bayesian Optimization frameworks. The main advantage of this function is that it has embedded both exploration and exploitation balance and can be computed in a closed form (without expensive numerical computations).\n",
    "\n",
    "Write a function that calculates the expected improvement given the posterior mean (m), posterior variance (v),\n",
    "and the value of the best point observed (best_y), for settings where we are maximizing.\n",
    "You can use either of these two equivalent formulas:\n",
    "\n",
    "$$\\mathrm{EI}_n(x) = \\sigma_n(x) \\varphi\\left(\\frac{\\Delta_n(x)}{\\sigma_n(x)}\\right) + \\Delta_n \\Phi\\left(\\frac{\\Delta_n(x)}{\\sigma_n(x)}\\right)$$\n",
    "\n",
    "\n",
    "$$\\mathrm{EI}_n(x) = [\\Delta_n(x)]^+ + \\sigma_n(x) \\varphi\\left(\\frac{\\Delta_n(x)}{\\sigma_n(x)}\\right) - |\\Delta_n|\\Phi\\left(\\frac{-|\\Delta_n(x)|}{\\sigma_n(x)}\\right)$$\n",
    "where\n",
    "- $\\mu_n(x)$ is the posterior mean at $x$\n",
    "- $\\sigma_n(x)$ is the posterior standard deviation at $x$\n",
    "- $\\varphi$ is the density of the standard normal distribution (i.e., normal distribution with mean 0 and variance 1)\n",
    "- $\\Phi$ is the cdf of the standard normal distribution\n",
    "- $\\Delta_n(x)$ is $\\mu_n(x) - f^*_n$ when we are maximizing and $f^*_n - \\mu_n(x)$ when we are minimizing, where $f^*_n$ is the value of the best point observed so far\n",
    "\n",
    "\n",
    "Your code may need to handle very small variances as a special case to avoid dividing by 0.\n",
    "The EI for a variance of 0 is max(0,m-best_y).  You may also need to handle small negative variances \n",
    "--- treat them as 0.\n",
    "\n",
    "You can use `scipy.stats.norm.pdf` and `scipy.stats.norm.cdf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-18T09:45:53.168316300Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "def EI(m:float,v:float,best_y:float, minimization:bool = False):\n",
    "    \n",
    "    '''\n",
    "    This is a base Expected Improvement function. The inputs of this function are:\n",
    "    - m: The posterior mean\n",
    "    - v: The posterior variance\n",
    "    - best_y: The sampled best value of the objective\n",
    "    - minimization: A boolean handle to determine if the objective is minimized or maximized\n",
    "    '''\n",
    "       \n",
    "        \n",
    "    ##### TODO: Enter your code here:\n",
    "\n",
    "    if v <= 0:\n",
    "        ret = max(0,m-best_y)\n",
    "    else:   \n",
    "        if minimization:\n",
    "            delta = best_y - m\n",
    "        else:\n",
    "            delta =  m - best_y\n",
    "            \n",
    "        s = np.sqrt(v)\n",
    "        z = delta / s\n",
    "        ret = max(0,delta)\n",
    "        ret += s * norm.pdf(z, loc=0) \n",
    "        ret -= abs(delta)*norm.cdf(-1*abs(z))\n",
    "    \n",
    "    return ret\n",
    "    \n",
    "    ##### --------------- END OF CODE INPUT -----------------------\n",
    "\n",
    "\n",
    "\n",
    "# Test your code!!!\n",
    "print(EI(1,1e-100,0))\n",
    "print(EI(2,1,0))\n",
    "print(EI(0,2,2))\n",
    "print(EI(0,10,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Acquistion Functions\n",
    "In the following block you will find the definition of other acquisition functions. Notice the inputs are mostly the same from before except for the last parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-18T09:45:53.170310500Z"
    }
   },
   "outputs": [],
   "source": [
    "def ucb(m:float,v:float,best_y:float, psi=1.96):\n",
    "    \"\"\"The upper confidence acquisition function.\"\"\"\n",
    "    if v <= 0:\n",
    "        return max(0,m-best_y)\n",
    "\n",
    "    return m + psi * np.sqrt(v)\n",
    "\n",
    "def lcb(m:float,v:float,best_y:float, psi=1.96):\n",
    "    \"\"\"The lower confidence acquisition function.\"\"\"\n",
    "    if v <= 0:\n",
    "        return max(0,m-best_y)\n",
    "    \n",
    "    return m - psi * np.sqrt(v)\n",
    "\n",
    "def poi(m:float,v:float,best_y:float, psi=0.):\n",
    "    \"\"\"Return the probability of improvement.\n",
    "    \n",
    "    Arguments\n",
    "    psi   -- A parameter that controls exploration.\n",
    "    \"\"\"\n",
    "    if v <= 0:\n",
    "        return max(0,m-best_y)\n",
    "    \n",
    "    return norm.cdf((m - best_y - psi) / np.sqrt(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Plotting an acquisition function\n",
    "Define a function `plot_acq` which uses the function defined above to compute it. In order to compute the variance, first use `GPR.fit(train_x,train_y)` to fit the model and then `GPR.predict(test_x, return_std=True)` to get the prediction of the mean and the prediction of the variance, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-18T09:45:53.172804900Z"
    }
   },
   "outputs": [],
   "source": [
    "kernel = Matern(length_scale=0.8,\n",
    "               length_scale_bounds=(1e-05,9e-01),\n",
    "               nu=1.5)\n",
    "# kernel = C(1.0, (1e-3, 1e3)) * RBF(length_scale=0.5, length_scale_bounds=(1e-2, 1e2))\n",
    "sampleGPR = GaussianProcessRegressor(kernel=kernel,\n",
    "                                    n_restarts_optimizer=10)\n",
    "\n",
    "sampleGPR.fit(X=train_x.reshape((-1,1)),\n",
    "                             y=train_y.reshape((-1,1)))\n",
    "\n",
    "# Using any acquistion function, we plot the posterior\n",
    "def plot_acq(train_x:np.ndarray,\n",
    "            train_y:np.ndarray,\n",
    "            best_y:float, \n",
    "            acq_fun:Callable=EI,\n",
    "            GPR:GaussianProcessRegressor=sampleGPR,\n",
    "            acq_fun_name:str = \"EI\"):\n",
    "    \n",
    "    if train_x.shape[0] == train_x.size:\n",
    "        train_x = train_x.reshape((-1,1))\n",
    "\n",
    "    if train_y.shape[0] == train_y.size:\n",
    "        train_y = train_y.reshape((-1,1))\n",
    "\n",
    "    best_y:np.ndarray = np.array([best_y]).reshape((-1,1))\n",
    "\n",
    "    ##### TODO:  Enter your code here:\n",
    "\n",
    "    # fit\n",
    "    GPR.fit(train_x,train_y)\n",
    "\n",
    "    acq_cum = [] # This list is to store the evaluated aquisition_function\n",
    "    \n",
    "    # This array is meant to sample equidistant points and evaluate the acquistion function\n",
    "    # At those points\n",
    "    x = np.linspace(0,1,100).reshape((-1,1)) \n",
    "\n",
    "    for test_x in x:\n",
    "        pred_mean, pred_std = GPR.predict(test_x.reshape((-1,1)), \n",
    "                                                               return_std=True)\n",
    "        acq = acq_fun(pred_mean.ravel(),np.power(pred_std.ravel(),2),best_y.ravel())\n",
    "        \n",
    "        acq_cum.append(acq)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6.5, 2))\n",
    "    ax.plot(x,acq_cum,'r',linewidth=1)\n",
    "    ax.legend([acq_fun_name])\n",
    "    ##### --------------- END OF CODE INPUT -----------------------\n",
    "\n",
    "best_y = max(train_y)\n",
    "print(train_y)\n",
    "print(best_y)\n",
    "\n",
    "\n",
    "plot_acq(train_x, train_y,best_y=best_y,GPR = sampleGPR)\n",
    "plot_prediction(train_x, train_y, gpr= sampleGPR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-18T09:45:53.176812400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Other plots\n",
    "plot_acq(train_x, train_y,best_y=best_y,GPR = sampleGPR,acq_fun=ucb,acq_fun_name=\"UCB\")\n",
    "plot_acq(train_x, train_y,best_y=best_y,GPR = sampleGPR,acq_fun=lcb,acq_fun_name=\"LCB\")\n",
    "plot_acq(train_x, train_y,best_y=best_y,GPR = sampleGPR,acq_fun=poi,acq_fun_name=\"POI\")\n",
    "plot_prediction(train_x, train_y, gpr= sampleGPR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Remarks and Thought Exercises\n",
    "1. Try changing the parameter `psi` in the case of all the Acquisition functions (all but Expected Improvement). Notice how this parameter controls the Exploration/Exploitation balance in each of the cases.\n",
    "2. Try changing the Kernel used for Exercise 5 (check the mentioned reference from `scikit-learn` library). Investigate how the shape of the acquisition functions change (in this case keep the values of `psi` constant). \n",
    "3. **What's your opinion???** Do you think the modularity of the Bayesian Optimization Pipeline is an advantage or disadvantage of the method?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_bo_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
